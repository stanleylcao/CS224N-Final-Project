{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fof import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing DeiTModel: ['cls_classifier.weight', 'distillation_classifier.bias', 'distillation_classifier.weight', 'cls_classifier.bias']\n",
      "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.bias', 'deit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.weight', 'h.7.crossattention.masked_bias', 'h.6.ln_cross_attn.weight', 'h.10.crossattention.masked_bias', 'h.6.crossattention.c_proj.bias', 'h.11.crossattention.bias', 'h.5.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.7.crossattention.bias', 'h.8.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.weight', 'h.6.crossattention.masked_bias', 'h.1.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.weight', 'h.9.ln_cross_attn.weight', 'h.0.crossattention.bias', 'h.10.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.masked_bias', 'h.8.crossattention.q_attn.weight', 'h.4.crossattention.bias', 'h.10.crossattention.c_proj.weight', 'h.6.crossattention.bias', 'h.1.crossattention.q_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.2.crossattention.bias', 'h.8.crossattention.masked_bias', 'h.0.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.2.crossattention.masked_bias', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.5.crossattention.masked_bias', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.10.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.9.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.1.crossattention.bias', 'h.6.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.masked_bias', 'h.8.crossattention.bias', 'h.10.crossattention.c_attn.weight', 'h.10.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.0.crossattention.masked_bias', 'h.5.crossattention.q_attn.weight', 'h.11.crossattention.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfiguring-out-figures\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/figuring-out-figures/figuring-out-figures/runs/3g4o0f5b\" target=\"_blank\">deit_test</a></strong> to <a href=\"https://wandb.ai/figuring-out-figures/figuring-out-figures\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | VisionEncoderDecoderModel | 239 M \n",
      "----------------------------------------------------\n",
      "239 M     Trainable params\n",
      "0         Non-trainable params\n",
      "239 M     Total params\n",
      "956.790   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ba65cb0bda4a54b74db157a5183df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fff0b4e0d7c40ffa106f77c9352b07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b2bbeeac61458898039182bbeb886e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "args = \"train --model deit_gpt --exp deit_test --batch_size 2 --limit 100 --lr 5e-5 --gpus 1 --caption_type normalized\".split()\n",
    "parser = run.get_parser(args)\n",
    "run.main(parser.parse_args(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "You are using a model of type clip to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_projection.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'visual_projection.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'logit_scale', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kevin/figuring-out-figures/exp.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bz840/home/kevin/figuring-out-figures/exp.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m args \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidate --pl_logger tb --model encdec --exp encdec-tpu-test-gpt2-validate --batch_size 32 --load_checkpoint ./checkpoints/tpu-test-gpt2/model/checkpoints/epoch=9-step=33389.ckpt --text_model gpt2\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39msplit() \u001b[39m# --resume_from_checkpoint  --gpus 0\".split()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bz840/home/kevin/figuring-out-figures/exp.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m parser \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39mget_parser(args)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bz840/home/kevin/figuring-out-figures/exp.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m run\u001b[39m.\u001b[39;49mmain(parser\u001b[39m.\u001b[39;49mparse_args(args))\n",
      "File \u001b[0;32m~/figuring-out-figures/fof/run.py:61\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=58'>59</a>\u001b[0m dict_args \u001b[39m=\u001b[39m \u001b[39mvars\u001b[39m(args)\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mmodel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mencdec\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='file:///~/figuring-out-figures/fof/run.py?line=60'>61</a>\u001b[0m     model \u001b[39m=\u001b[39m EncoderDecoderModel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdict_args)\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=62'>63</a>\u001b[0m datamodule \u001b[39m=\u001b[39m ScicapDataModule(\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=63'>64</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFirst-Sentence\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=64'>65</a>\u001b[0m     batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=65'>66</a>\u001b[0m     limit\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mlimit,\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=66'>67</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mtext_tokenizer,\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=67'>68</a>\u001b[0m     caption_type\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcaption_type)\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/run.py?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/figuring-out-figures/fof/encdec.py:27\u001b[0m, in \u001b[0;36mEncoderDecoderModel.__init__\u001b[0;34m(self, text_model, lr, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=25'>26</a>\u001b[0m encoder \u001b[39m=\u001b[39m ExtensibleEncoder()\n\u001b[0;32m---> <a href='file:///~/figuring-out-figures/fof/encdec.py?line=26'>27</a>\u001b[0m decoder \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39;49mAutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=27'>28</a>\u001b[0m     text_model, add_cross_attention\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=29'>30</a>\u001b[0m model \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39mVisionEncoderDecoderModel(\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=30'>31</a>\u001b[0m     encoder\u001b[39m=\u001b[39mencoder\u001b[39m.\u001b[39mclip, decoder\u001b[39m=\u001b[39mdecoder)\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=31'>32</a>\u001b[0m \u001b[39m# model.encoder = encoder\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=32'>33</a>\u001b[0m \u001b[39m# use GPT2's eos_token as the pad as well as eos token\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/figuring-out-figures/fof/encdec.py?line=33'>34</a>\u001b[0m \u001b[39m# TODO is this line correct?\u001b[39;00m\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py:447\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=444'>445</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=445'>446</a>\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=447'>448</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=448'>449</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=449'>450</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/auto/auto_factory.py?line=450'>451</a>\u001b[0m )\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/modeling_utils.py:1493\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1490'>1491</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1491'>1492</a>\u001b[0m     \u001b[39mwith\u001b[39;00m no_init_weights(_enable\u001b[39m=\u001b[39m_fast_init):\n\u001b[0;32m-> <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1492'>1493</a>\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1494'>1495</a>\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1495'>1496</a>\u001b[0m     \u001b[39m# restore default dtype\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1496'>1497</a>\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py:951\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=948'>949</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=949'>950</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=950'>951</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPT2Model(config)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=951'>952</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=953'>954</a>\u001b[0m     \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py:680\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=676'>677</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=678'>679</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=679'>680</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=680'>681</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=682'>683</a>\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py:680\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=676'>677</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=678'>679</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=679'>680</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39;49mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=680'>681</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=682'>683</a>\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py:380\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=376'>377</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m GPT2Attention(config, is_cross_attention\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, layer_idx\u001b[39m=\u001b[39mlayer_idx)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=377'>378</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_cross_attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=379'>380</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m GPT2MLP(inner_dim, config)\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py:353\u001b[0m, in \u001b[0;36mGPT2MLP.__init__\u001b[0;34m(self, intermediate_size, config)\u001b[0m\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=350'>351</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=351'>352</a>\u001b[0m embed_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m--> <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=352'>353</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc \u001b[39m=\u001b[39m Conv1D(intermediate_size, embed_dim)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=353'>354</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj \u001b[39m=\u001b[39m Conv1D(embed_dim, intermediate_size)\n\u001b[1;32m    <a href='file:///~/figuring-out-figures/transformers/src/transformers/models/gpt2/modeling_gpt2.py?line=354'>355</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mactivation_function]\n",
      "File \u001b[0;32m~/figuring-out-figures/transformers/src/transformers/modeling_utils.py:1861\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[0;34m(self, nf, nx)\u001b[0m\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1858'>1859</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf \u001b[39m=\u001b[39m nf\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1859'>1860</a>\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(nx, nf)\n\u001b[0;32m-> <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1860'>1861</a>\u001b[0m nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mnormal_(w, std\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m)\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1861'>1862</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(w)\n\u001b[1;32m   <a href='file:///~/figuring-out-figures/transformers/src/transformers/modeling_utils.py?line=1862'>1863</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(nf))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py:151\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=137'>138</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormal_\u001b[39m(tensor: Tensor, mean: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, std: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=138'>139</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Fills the input Tensor with values drawn from the normal\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=139'>140</a>\u001b[0m \u001b[39m    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=140'>141</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=148'>149</a>\u001b[0m \u001b[39m        >>> nn.init.normal_(w)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=149'>150</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=150'>151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=17'>18</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='file:///~/.local/share/virtualenvs/figuring-out-figures-rn5smVdW/lib/python3.8/site-packages/torch/nn/init.py?line=18'>19</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Validate the TPU-trained GPT2 model for 9 epochs\n",
    "args = \"validate --pl_logger tb --model encdec --exp encdec-tpu-test-gpt2-validate --batch_size 32 --load_checkpoint ./checkpoints/tpu-test-gpt2/model/checkpoints/epoch=9-step=33389.ckpt --text_model gpt2\".split() # --resume_from_checkpoint  --gpus 0\".split()\n",
    "parser = run.get_parser(args)\n",
    "run.main(parser.parse_args(args))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbd50cbcf196040c3361f94de74ab67749b25e306e24920f52a359bc6db0f798"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('figuring-out-figures-rn5smVdW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
